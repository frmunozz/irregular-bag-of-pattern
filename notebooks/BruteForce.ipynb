{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brute force algorithm\n",
    "\n",
    "naive implementation using DTW and TWED distances as a control case for future implementations.\n",
    "\n",
    "Here we will compute distance matrix for a given dataset and save the result to disk. Then, using this matrix we will evaluate performance on 1-nearest neighbor classification and Average Mean Presicion at K (AP@K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial import distance\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import multiprocessing as mp\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset generation\n",
    "\n",
    "first we create a small dataset used for testing on which we reduce the number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full PLaSTiCC dataset size:  7848\n",
      "number of classes/targets:  14\n",
      "classes count:\n",
      ">> class:  90 , count:  2313\n",
      ">> class:  42 , count:  1193\n",
      ">> class:  65 , count:  981\n",
      ">> class:  16 , count:  924\n",
      ">> class:  15 , count:  495\n",
      ">> class:  62 , count:  484\n",
      ">> class:  88 , count:  370\n",
      ">> class:  92 , count:  239\n",
      ">> class:  67 , count:  208\n",
      ">> class:  52 , count:  183\n",
      ">> class:  95 , count:  175\n",
      ">> class:  6 , count:  151\n",
      ">> class:  64 , count:  102\n",
      ">> class:  53 , count:  30\n"
     ]
    }
   ],
   "source": [
    "def load_pandas(path,  **kwargs):\n",
    "    data_filename = kwargs.pop(\"data_filename\")\n",
    "    meta_filename = kwargs.pop(\"meta_filename\")\n",
    "    df = pd.read_csv(path + data_filename)\n",
    "    df_metadata = pd.read_csv(path + meta_filename)\n",
    "\n",
    "    passband_id = 3\n",
    "\n",
    "    df = df[df[\"passband\"] == passband_id]\n",
    "    df = df.sort_values(by=[\"object_id\", \"mjd\"])\n",
    "    df_metadata = df_metadata.sort_values(by=[\"object_id\"])\n",
    "    df = df.groupby(\"object_id\")\n",
    "    fluxes = df['flux'].apply(list)\n",
    "    times = df['mjd'].apply(list)\n",
    "    ids = df.groups.keys()\n",
    "    dataset = [np.array(fluxes.loc[i]) for i in ids]\n",
    "    times_arr = []\n",
    "    for i in ids:\n",
    "        times_i = np.array(times.loc[i])\n",
    "        times_i = times_i - times_i[0]\n",
    "        times_arr.append(times_i)\n",
    "\n",
    "    labels = df_metadata[\"target\"].to_numpy()\n",
    "\n",
    "    return dataset, times_arr, labels, len(dataset)\n",
    "\n",
    "def adjust_labels(labels):\n",
    "    classes = np.sort(np.unique(labels))\n",
    "    classes_count = np.zeros(len(classes), dtype=int)\n",
    "    label_index = np.zeros(m)\n",
    "    for i, l in enumerate(labels):\n",
    "        position = np.where(classes == l)[0][0]\n",
    "        classes_count[position] += 1\n",
    "        label_index[i] = position\n",
    "\n",
    "    count_sort_best_index = np.argsort(classes_count)[::-1]\n",
    "    \n",
    "    return classes_count, classes, label_index, count_sort_best_index\n",
    "\n",
    "path = \"/home/frmunoz/Documents/tesis/data/\"\n",
    "dataset, times, labels, m = load_pandas(path, data_filename=\"training_set.csv.zip\", meta_filename=\"training_set_metadata.csv\")\n",
    "class_count, lclasses, label_index, sort_class_count_index = adjust_labels(labels)\n",
    "\n",
    "print(\"full PLaSTiCC dataset size: \", m)\n",
    "print(\"number of classes/targets: \", np.unique(labels).size)\n",
    "print(\"classes count:\")\n",
    "for i in sort_class_count_index:\n",
    "    print(\">> class: \", lclasses[i], \", count: \", class_count[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_small_dataset(dataset, times, labels, sort_class_count_index, classes, m, n1, n2, c):\n",
    "    # n: size of the small dataset\n",
    "    # c: use the 'c' classes most present in the original dataset\n",
    "    \n",
    "    classes_cut = []\n",
    "    for i in sort_class_count_index[:c]:\n",
    "        classes_cut.append(classes[i])\n",
    "    \n",
    "    data_time_tuple = []\n",
    "    labels_cut = []\n",
    "    for t, y, l in zip(times, dataset, labels):\n",
    "        if l in classes_cut:\n",
    "            data_time_tuple.append((t, y))\n",
    "            labels_cut.append(l)\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data_time_tuple, labels_cut,\n",
    "                                                    test_size=len(labels_cut) - n1,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=labels_cut)\n",
    "    \n",
    "    dataset_split = []\n",
    "    times_split = []\n",
    "    for d_tuple in x_train:\n",
    "        times_split.append(d_tuple[0])\n",
    "        dataset_split.append(d_tuple[1])\n",
    "            \n",
    "    \n",
    "    if n2 < len(y_test) - 1:\n",
    "        \n",
    "        _, x_test, _, y_test = train_test_split(x_test, y_test,\n",
    "                                            test_size=n2,\n",
    "                                            random_state=0,\n",
    "                                            stratify=y_test)\n",
    "        \n",
    "    d_test_split = []\n",
    "    t_test_split = []\n",
    "    for d_tuple in x_test:\n",
    "        d_test_split.append(d_tuple[1])\n",
    "        t_test_split.append(d_tuple[0])\n",
    "    \n",
    "    return dataset_split, times_split, y_train, d_test_split, t_test_split, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> SKIP: dataset of size: 10 train, 10 test, with 2 classes already exists\n",
      ">>> SKIP: dataset of size: 100 train, 100 test, with 4 classes already exists\n",
      ">>> SKIP: dataset of size: 200 train, 100 test, with 4 classes already exists\n",
      ">>> SKIP: dataset of size: 500 train, 500 test, with 4 classes already exists\n",
      ">>> SKIP: dataset of size: 1000 train, 500 test, with 4 classes already exists\n",
      ">>> SKIP: dataset of size: 1000 train, 1000 test, with 4 classes already exists\n",
      ">>> SKIP: dataset of size: 1000 train, 1000 test, with 10 classes already exists\n",
      ">>> SKIP: dataset of size: 2000 train, 1000 test, with 14 classes already exists\n",
      ">>> SKIP: dataset of size: 2000 train, 2000 test, with 14 classes already exists\n",
      ">>> SKIP: dataset of size: 5000 train, 2847 test, with 14 classes already exists\n",
      ">>> SKIP: dataset of size: 6278 train, 1570 test, with 14 classes already exists\n"
     ]
    }
   ],
   "source": [
    "n1_arr = [10, 100, 200, 500, 1000, 1000, 1000, 2000, 2000, 5000,  int(m * 0.8)]\n",
    "n2_arr = [10, 100, 100, 500, 500, 1000, 1000, 1000, 2000, 2847, m - int(m * 0.8)]\n",
    "c_arr = [2, 4, 4, 4, 4, 4, 10, len(lclasses), len(lclasses), len(lclasses), len(lclasses)]\n",
    "\n",
    "\n",
    "datasets_folder = \"/home/frmunoz/Documents/tesis/data/plasticc_sub_dataset/\"\n",
    "train_d_smalls = []\n",
    "train_t_smalls = []\n",
    "train_l_smalls = []\n",
    "test_d_smalls = []\n",
    "test_t_smalls = []\n",
    "test_l_smalls = []\n",
    "\n",
    "for c, n1, n2 in zip(c_arr, n1_arr, n2_arr):\n",
    "    D_s1, t_s1, l_s1, D_s1_test, t_s1_test, l_s1_test = get_small_dataset(dataset, times, labels, \n",
    "                                                                          sort_class_count_index, lclasses,\n",
    "                                                                          m, n1, n2, c)\n",
    "    train_d_smalls.append(D_s1)\n",
    "    train_t_smalls.append(t_s1)\n",
    "    train_l_smalls.append(l_s1)\n",
    "    test_d_smalls.append(D_s1_test)\n",
    "    test_t_smalls.append(t_s1_test)\n",
    "    test_l_smalls.append(l_s1_test)\n",
    "    \n",
    "    # save to disk\n",
    "    skip = False\n",
    "    for type_i, data_i in zip([\"d\", \"t\", \"l\"],\n",
    "                             [(D_s1, D_s1, D_s1_test), (t_s1, t_s1_test), (l_s1, l_s1_test)]):\n",
    "        out_file_train = datasets_folder + \"train_{}_n{}_c{}.npy\".format(type_i, n1, c)\n",
    "        out_file_test = datasets_folder + \"test_{}_n{}_c{}.npy\".format(type_i, n2, c)\n",
    "        if os.path.exists(out_file_train) and os.path.exists(out_file_test):\n",
    "            print(\">>> SKIP: dataset of size:\", n1, \"train,\", n2, \"test, with\", c, \"classes already exists\")\n",
    "            skip = True\n",
    "            break\n",
    "        else:\n",
    "            np.save(out_file_train, data_i[0])\n",
    "            np.save(out_file_test, data_i[1])\n",
    "            \n",
    "    if not skip:\n",
    "        print(\":::GEN train dataset of size:\", n1, \"and test dataset of size\",  n2, \"with\", c, \"classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW\n",
    "\n",
    "compute distance matrix between test and train datasets using DTW distance from [here](https://github.com/pierre-rouanet/dtw)\n",
    "\n",
    "**very expensive executions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dtw import dtw, accelerated_dtw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_matrix(X_train, X_test):\n",
    "    # compute distances between each time series on test set and train set\n",
    "    n = len(X_train)\n",
    "    m = len(X_test)\n",
    "    res = np.ones((n, m)) * -1\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            d, cost_matrix, acc_cost_matrix, path = accelerated_dtw(X_train[i], X_test[j], 'cosine')\n",
    "            res[i][j] = d\n",
    "    return res\n",
    "\n",
    "def worker(X_train, X_test, i_subset, j_subset, out_q):\n",
    "    try:\n",
    "        n = len(X_train)\n",
    "        m = len(X_test)\n",
    "        rows = np.ones((n,m)) *  -1\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                d, cost_matrix, acc_cost_matrix, path = accelerated_dtw(X_train[i], X_test[j], 'cosine')\n",
    "                rows[i][j] = d\n",
    "        out_q.put((i_subset, j_subset, rows))\n",
    "    except:\n",
    "        print(\"worker failed\")\n",
    "    finally:\n",
    "        print(\"done\")\n",
    "\n",
    "# this code doesnt work on ipython notebooks when using windows system.\n",
    "def dmatrix_multiprocessing(X_train, X_test, n_process):\n",
    "    n = len(X_train)\n",
    "    m = len(X_test)\n",
    "    n_subset = n // n_process\n",
    "    man = mp.Manager()\n",
    "    result_queue = man.Queue()\n",
    "    jobs = []\n",
    "    for k in range(n_process):\n",
    "        i_subset = k * n_subset\n",
    "        j_subset = (k+1) * n_subset\n",
    "        if j_subset > n:\n",
    "            j_subset = n\n",
    "        X_train_subset = X_train[i_subset:j_subset]\n",
    "        jobs.append(mp.Process(target=worker, \n",
    "                               args=(X_train_subset, X_test, i_subset, j_subset, result_queue)))\n",
    "        jobs[-1].start()\n",
    "    \n",
    "    for p in jobs:\n",
    "        p.join()\n",
    "    \n",
    "    dmatrix = np.zeros((n, m))\n",
    "    num_res = result_queue.size()\n",
    "    while num_ress > 0:\n",
    "        i_subset, j_subset, rows_subset = result_queue.get()\n",
    "        dmatrix[i_subset:j_subset] = rows_subset\n",
    "    \n",
    "    return dmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 333 ms, sys: 650 µs, total: 334 ms\n",
      "Wall time: 330 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dataset train size 10, test size 10, 2 classes\n",
    "d_matrix_10_10_2 = distance_matrix(train_d_smalls[0], test_d_smalls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 s, sys: 52 ms, total: 24.6 s\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# dataset train size 100, test size 100, 4 classes\n",
    "d_matrix_100_100_4 = distance_matrix(train_d_smalls[1], test_d_smalls[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for more big datasets, please run the multiprocessing algorithm from terminal, here we will just load them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-nearest neighbor classification\n",
    "\n",
    "having the distance matrix we will classify each time series on the test set using the training set. \n",
    "\n",
    "The procedure will be:\n",
    "\n",
    "1. For each column on the distance matrix, get the clossest distance\n",
    "2. Check training and test label looking for match. \n",
    "3. Final accuracy will be the number of matches divided by the total number of test samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruteforce_classifier(dataset_folder, dmatrix_folder, n1, n2, c):\n",
    "    dmatrix = np.load(dmatrix_folder + \"dmatrix_n{}_m{}_c{}.npy\".format(n1, n2, c), allow_pickle=True)\n",
    "    train_labels = np.load(dataset_folder + \"train_l_n{}_c{}.npy\".format(n1, c), allow_pickle=True)\n",
    "    test_labels = np.load(dataset_folder + \"test_l_n{}_c{}.npy\".format(n2, c), allow_pickle=True)\n",
    "    \n",
    "    n = len(train_labels)\n",
    "    m = len(test_labels)\n",
    "    count = 0\n",
    "    for j in range(m):\n",
    "        dmin = np.inf\n",
    "        mink = -1\n",
    "        for i in range(n):\n",
    "            if dmatrix[i][j] < dmin:\n",
    "                dmin = dmatrix[i][j]\n",
    "                mink = i\n",
    "        if test_labels[j] == train_labels[mink]:\n",
    "            count += 1\n",
    "    return count, count/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/home/frmunoz/Documents/tesis/data/plasticc_sub_dataset/\"\n",
    "path2 = \"/home/frmunoz/Documents/tesis/data/bruteforce_dmatrix/\"\n",
    "count, acc = bruteforce_classifier(path1, path2, 1000, 1000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.245"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision at K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(target_test, targets_train, predicted_targets_idx):\n",
    "    \n",
    "    n_tp_total = np.sum([target_test == targets_train[i] for i in predicted_targets_idx])\n",
    "    if n_tp_total == 0:\n",
    "        return 0\n",
    "    k = len(predicted_targets_idx)\n",
    "    ap_sum = 0\n",
    "    for i in range(k):\n",
    "        tp_seen = 0\n",
    "        if target_test == targets_train[predicted_targets_idx[i]]:\n",
    "            for j in range(i+1):\n",
    "                if target_test == targets_train[predicted_targets_idx[j]]:\n",
    "                    tp_seen += 1\n",
    "        ap_sum += tp_seen / (i+1)\n",
    "    return ap_sum / n_tp_total\n",
    "\n",
    "def m_av_p_k(dataset_folder, dmatrix_folder, n1, n2, c, k):\n",
    "    dmatrix = np.load(dmatrix_folder + \"dmatrix_n{}_m{}_c{}.npy\".format(n1, n2, c), allow_pickle=True)\n",
    "    train_labels = np.load(dataset_folder + \"train_l_n{}_c{}.npy\".format(n1, c), allow_pickle=True)\n",
    "    test_labels = np.load(dataset_folder + \"test_l_n{}_c{}.npy\".format(n2, c), allow_pickle=True)\n",
    "    \n",
    "    n = len(train_labels)\n",
    "    m = len(test_labels)\n",
    "    mapk = []\n",
    "    for j in range(m):\n",
    "        best_idx_sort = np.argsort(dmatrix[:,j])[::-1]\n",
    "        best_idx_sort_k = best_idx_sort[:k]\n",
    "        apk = average_precision(test_labels[j], train_labels, best_idx_sort_k)\n",
    "        mapk.append(apk)\n",
    "    \n",
    "    return np.sum(mapk)/ m, mapk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk, vec = m_av_p_k(path1, path2, 6278, 1570, 14, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17772945221967063"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TWED\n",
    "\n",
    "compute distance matrix between test and train datasets using TWED distance from [here](https://en.wikipedia.org/wiki/Time_Warp_Edit_Distance)\n",
    "\n",
    "elasticity parameter **nu** and penalty paramete **\\_lambda** are set to 1 and 0.001 respectively in an arbitrary way (should be fit)\n",
    "\n",
    "**very expensive executions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dlp(A, B, p=2):\n",
    "    cost = np.sum(np.power(np.abs(A - B), p))\n",
    "    return np.power(cost, 1 / p)\n",
    "\n",
    "\n",
    "def twed(A, timeSA, B, timeSB, nu, _lambda):\n",
    "    # [distance, DP] = TWED( A, timeSA, B, timeSB, lambda, nu )\n",
    "    # Compute Time Warp Edit Distance (TWED) for given time series A and B\n",
    "    #\n",
    "    # A      := Time series A (e.g. [ 10 2 30 4])\n",
    "    # timeSA := Time stamp of time series A (e.g. 1:4)\n",
    "    # B      := Time series B\n",
    "    # timeSB := Time stamp of time series B\n",
    "    # lambda := Penalty for deletion operation\n",
    "    # nu     := Elasticity parameter - nu >=0 needed for distance measure\n",
    "    # Reference :\n",
    "    #    Marteau, P.; F. (2009). \"Time Warp Edit Distance with Stiffness Adjustment for Time Series Matching\".\n",
    "    #    IEEE Transactions on Pattern Analysis and Machine Intelligence. 31 (2): 306–318. arXiv:cs/0703033\n",
    "    #    http://people.irisa.fr/Pierre-Francois.Marteau/\n",
    "\n",
    "    # Check if input arguments\n",
    "    if len(A) != len(timeSA):\n",
    "        print(\"The length of A is not equal length of timeSA\")\n",
    "        return None, None\n",
    "\n",
    "    if len(B) != len(timeSB):\n",
    "        print(\"The length of B is not equal length of timeSB\")\n",
    "        return None, None\n",
    "\n",
    "    if nu < 0:\n",
    "        print(\"nu is negative\")\n",
    "        return None, None\n",
    "\n",
    "    # Add padding\n",
    "    A = np.array([0] + list(A))\n",
    "    timeSA = np.array([0] + list(timeSA))\n",
    "    B = np.array([0] + list(B))\n",
    "    timeSB = np.array([0] + list(timeSB))\n",
    "\n",
    "    n = len(A)\n",
    "    m = len(B)\n",
    "    # Dynamical programming\n",
    "    DP = np.zeros((n, m))\n",
    "\n",
    "    # Initialize DP Matrix and set first row and column to infinity\n",
    "    DP[0, :] = np.inf\n",
    "    DP[:, 0] = np.inf\n",
    "    DP[0, 0] = 0\n",
    "\n",
    "    # Compute minimal cost\n",
    "    for i in range(1, n):\n",
    "        for j in range(1, m):\n",
    "            # Calculate and save cost of various operations\n",
    "            C = np.ones((3, 1)) * np.inf\n",
    "            # Deletion in A\n",
    "            C[0] = (\n",
    "                DP[i - 1, j]\n",
    "                + Dlp(A[i - 1], A[i])\n",
    "                + nu * (timeSA[i] - timeSA[i - 1])\n",
    "                + _lambda\n",
    "            )\n",
    "            # Deletion in B\n",
    "            C[1] = (\n",
    "                DP[i, j - 1]\n",
    "                + Dlp(B[j - 1], B[j])\n",
    "                + nu * (timeSB[j] - timeSB[j - 1])\n",
    "                + _lambda\n",
    "            )\n",
    "            # Keep data points in both time series\n",
    "            C[2] = (\n",
    "                DP[i - 1, j - 1]\n",
    "                + Dlp(A[i], B[j])\n",
    "                + Dlp(A[i - 1], B[j - 1])\n",
    "                + nu * (abs(timeSA[i] - timeSB[j]) + abs(timeSA[i - 1] - timeSB[j - 1]))\n",
    "            )\n",
    "            # Choose the operation with the minimal cost and update DP Matrix\n",
    "            DP[i, j] = np.min(C)\n",
    "    distance = DP[n - 1, m - 1]\n",
    "    return distance, DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935.1264900000931"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, dp = twed(train_d_smalls[0][0], train_t_smalls[0][0], test_d_smalls[0][0], test_t_smalls[0][0], 1, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.ones((5, 8))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1602872992.0076933"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.287370681762695"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b - a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
